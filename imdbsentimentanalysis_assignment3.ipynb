{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "KECEoha17LxS",
        "outputId": "19687a06-393b-4e34-d435-1f93d6701939"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "P1VmYnoH7Lw-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yT9ZV3bJS1sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5P0Xkw0auGf"
      },
      "source": [
        "# Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDkdTzQIauGg"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Research methods in data science/Assignment 3/IMDB Dataset.csv'\n",
        "df=pd.read_csv(data_path)\n",
        "df.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYnTu5T37Lxc"
      },
      "source": [
        "## Installing Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aISTcEi07Lxf"
      },
      "outputs": [],
      "source": [
        "# Installing Transformers library\n",
        "! pip install -q transformers==4.17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW7qaFr1auGg"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugehwcse7Lxi"
      },
      "outputs": [],
      "source": [
        "# Loading the BERT Classifier and Tokenizer along with Input module\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMh7YN-WauGh"
      },
      "source": [
        "## Summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-XRkKja7Lxk"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU6z5OoIauGh"
      },
      "source": [
        "## Encoding Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BERsDKaw7Lxn"
      },
      "outputs": [],
      "source": [
        "# changing positive and negative into numeric values\n",
        "\n",
        "def cat2num(value):\n",
        "    if value=='positive':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['sentiment']  =  df['sentiment'].apply(cat2num)\n",
        "train = df[:45000]\n",
        "test = df[45000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7bj-82u7Lxp"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrM9R9l17Lxq"
      },
      "outputs": [],
      "source": [
        "\n",
        "example='In this Kaggle notebook, I will do sentiment analysis using BERT with Huggingface'\n",
        "tokens=tokenizer.tokenize(example)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_BmkNiX7Lxs"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_examples(train, test, review, sentiment):\n",
        "    train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[review],\n",
        "                                                          label = x[sentiment]), axis = 1)\n",
        "\n",
        "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[review],\n",
        "                                                          label = x[sentiment]), axis = 1,)\n",
        "\n",
        "    return train_InputExamples, validation_InputExamples\n",
        "\n",
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taU5X57v7Lxt"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> will hold InputFeatures to be converted later\n",
        "\n",
        "    for e in tqdm(examples):\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label) )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "DATA_COLUMN = 'review'\n",
        "LABEL_COLUMN = 'sentiment'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdKFJrHauGi"
      },
      "source": [
        "## Creating Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEG3G3bI7Lxw"
      },
      "outputs": [],
      "source": [
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Cprz3nauGi"
      },
      "source": [
        "## Creating Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3B3GL3T7Lxw"
      },
      "outputs": [],
      "source": [
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omdnzg1BauGi"
      },
      "source": [
        "# Compiling and Fitting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxuAaLFs7Lxx"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "model.fit(train_data, epochs=2, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97dDTaGauGi"
      },
      "source": [
        "# Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkFrsqooPMt8"
      },
      "outputs": [],
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70kMRijLauGi"
      },
      "source": [
        "# Testing the Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcRMepsN7Lxy"
      },
      "outputs": [],
      "source": [
        "pred_sentences = ['worst movie of my life, will never watch movies from this series', 'Wow, blew my mind, what a movie by Marvel, animation and story is amazing']\n",
        "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n",
        "tf_outputs = model(tf_batch)\n",
        "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n",
        "labels = ['Negative','Positive']\n",
        "label = tf.argmax(tf_predictions, axis=1)\n",
        "label = label.numpy()\n",
        "for i in range(len(pred_sentences)):\n",
        "    print(pred_sentences[i], \": \", labels[label[i]])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}